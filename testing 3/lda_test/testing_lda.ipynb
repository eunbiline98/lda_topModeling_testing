{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (6.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyldavis==3.2.1 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: numpy>=1.9.2 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyldavis==3.2.1) (1.21.5)\n",
      "Requirement already satisfied: future in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyldavis==3.2.1) (0.18.2)\n",
      "Requirement already satisfied: funcy in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyldavis==3.2.1) (1.17)\n",
      "Requirement already satisfied: numexpr in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyldavis==3.2.1) (2.8.4)\n",
      "Requirement already satisfied: pandas>=0.17.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyldavis==3.2.1) (1.1.5)\n",
      "Requirement already satisfied: scipy>=0.18.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyldavis==3.2.1) (1.7.3)\n",
      "Requirement already satisfied: wheel>=0.23.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyldavis==3.2.1) (0.37.1)\n",
      "Requirement already satisfied: joblib>=0.8.4 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyldavis==3.2.1) (1.2.0)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyldavis==3.2.1) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jinja2>=2.7.2->pyldavis==3.2.1) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas>=0.17.0->pyldavis==3.2.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas>=0.17.0->pyldavis==3.2.1) (2022.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=0.17.0->pyldavis==3.2.1) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pyldavis==3.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Sastrawi in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swifter in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: dask[dataframe]>=2.10.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from swifter) (2022.2.0)\n",
      "Requirement already satisfied: parso>0.4.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from swifter) (0.8.3)\n",
      "Requirement already satisfied: bleach>=3.1.1 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from swifter) (5.0.1)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from swifter) (2.2.0)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from swifter) (1.1.5)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from swifter) (4.64.1)\n",
      "Requirement already satisfied: psutil>=5.6.6 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from swifter) (5.9.0)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from swifter) (8.0.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from bleach>=3.1.1->swifter) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from bleach>=3.1.1->swifter) (0.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (21.3)\n",
      "Requirement already satisfied: partd>=0.3.10 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (1.3.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (0.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (6.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (2022.11.0)\n",
      "Requirement already satisfied: numpy>=1.18 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (1.21.5)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (4.0.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (7.34.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (3.0.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (5.6.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (6.16.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas>=1.0.0->swifter) (2022.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tqdm>=4.33.0->swifter) (0.4.4)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (1.5.6)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (7.4.7)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (1.6.4)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (6.2)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (24.0.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (63.4.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.18.2)\n",
      "Requirement already satisfied: backcall in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.2.0)\n",
      "Requirement already satisfied: pygments in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (2.13.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (3.0.33)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from packaging>=20.0->dask[dataframe]>=2.10.0->swifter) (3.0.9)\n",
      "Requirement already satisfied: locket in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from partd>=0.3.10->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (4.12.0)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (0.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.2.5)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (304)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install swifter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Library Read File Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Folding Result : \n",
      "\n",
      "0     website yang menarik dan tentunya mudah dipaha...\n",
      "1                                    variatif dan bagus\n",
      "2                                          keren banget\n",
      "3     kurang responsive di bagian menu sebelah kanan...\n",
      "4     sudah lumayan baik dan responsif. namun di bag...\n",
      "5                                            biasa saja\n",
      "6                                        lumayang bagus\n",
      "7                                     b aja, kadang bug\n",
      "8             desainnya cukup menarik dan user friendly\n",
      "9     pendapat saya tentang website tersebut, cukup ...\n",
      "10                                         bagus sekali\n",
      "11    sejauh ini selama saya pernah mengakses websit...\n",
      "12    letak navbar kurang efektif jika berada disebe...\n",
      "13                                                 baik\n",
      "14                                        kurang update\n",
      "15                                                 baik\n",
      "16                                           cukup baik\n",
      "17        website yang tampilannya kekinian dan modern.\n",
      "18                                                bagus\n",
      "19                                    sangat bermanfaat\n",
      "20          bagus, sangat bermanfaat dan menginspirasi \n",
      "21                                           interaktif\n",
      "22                                                bagus\n",
      "23    pendapat saya tentang kampus institut asia mal...\n",
      "24    website institut asia, sangat bagus dan simple...\n",
      "Name: Question1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "dataSB = pd.read_excel('/Users/thaar/Downloads/lda_topModeling_testing-main (2)/lda_topModeling_testing-main/testing 3/lda_test/content/dataKuisoner.xlsx', engine='openpyxl')  # lokasi file\n",
    "dataSB.head()\n",
    "dataSB['Question1'] = dataSB['Question1'].str.lower()\n",
    "\n",
    "print('Case Folding Result : \\n')\n",
    "print(dataSB['Question1'].head(25))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import re #regex library\n",
    "\n",
    "# import word_tokenize & FreqDist from NLTK\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Result : \n",
      "\n",
      "0    [website, yang, menarik, dan, tentunya, mudah,...\n",
      "1                               [variatif, dan, bagus]\n",
      "2                                      [keren, banget]\n",
      "3    [kurang, responsive, di, bagian, menu, sebelah...\n",
      "4    [sudah, lumayan, baik, dan, responsif, namun, ...\n",
      "Name: Question1_tokens, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\thaar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ------ Tokenizing ---------\n",
    "\n",
    "nltk.download('punkt')\n",
    "def remove_tweet_special(text):\n",
    "    # remove tab, new line, ans back slice\n",
    "    text = str(text).replace('\\\\t', \" \").replace(\n",
    "        '\\\\n', \" \").replace('\\\\u', \" \").replace('\\\\', \"\")\n",
    "    # remove non ASCII (emoticon, chinese word, .etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # remove mention, link, hashtag\n",
    "    text = ' '.join(\n",
    "        re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
    "    # remove incomplete URL\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "\n",
    "\n",
    "dataSB['Question1'] = dataSB['Question1'].apply(remove_tweet_special)\n",
    "\n",
    "# remove number\n",
    "\n",
    "\n",
    "def remove_number(text):\n",
    "    return re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "\n",
    "dataSB['Question1'] = dataSB['Question1'].apply(remove_number)\n",
    "\n",
    "# remove punctuation\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "\n",
    "dataSB['Question1'] = dataSB['Question1'].apply(remove_punctuation)\n",
    "\n",
    "# remove whitespace leading & trailing\n",
    "\n",
    "\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "dataSB['Question1'] = dataSB['Question1'].apply(remove_whitespace_LT)\n",
    "\n",
    "# remove multiple whitespace into single whitespace\n",
    "\n",
    "\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+', ' ', text)\n",
    "\n",
    "\n",
    "dataSB['Question1'] = dataSB['Question1'].apply(remove_whitespace_multiple)\n",
    "\n",
    "# remove single char\n",
    "\n",
    "\n",
    "def remove_singl_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "\n",
    "dataSB['Question1'] = dataSB['Question1'].apply(remove_singl_char)\n",
    "\n",
    "# NLTK word tokenize\n",
    "\n",
    "\n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "dataSB['Question1_tokens'] = dataSB['Question1'].apply(word_tokenize_wrapper)\n",
    "\n",
    "print('Tokenizing Result : \\n')\n",
    "print(dataSB['Question1_tokens'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Tokens : \n",
      "\n",
      "0     [(website, 1), (yang, 1), (menarik, 1), (dan, ...\n",
      "1                 [(variatif, 1), (dan, 1), (bagus, 1)]\n",
      "2                             [(keren, 1), (banget, 1)]\n",
      "3     [(di, 2), (putih, 2), (kurang, 1), (responsive...\n",
      "4     [(baik, 2), (sudah, 1), (lumayan, 1), (dan, 1)...\n",
      "5                               [(biasa, 1), (saja, 1)]\n",
      "6                           [(lumayang, 1), (bagus, 1)]\n",
      "7                     [(aja, 1), (kadang, 1), (bug, 1)]\n",
      "8     [(desainnya, 1), (cukup, 1), (menarik, 1), (da...\n",
      "9     [(tentang, 2), (website, 2), (tersebut, 2), (y...\n",
      "10                            [(bagus, 1), (sekali, 1)]\n",
      "11    [(sudah, 3), (lengkap, 2), (sejauh, 1), (ini, ...\n",
      "12    [(navbar, 2), (kurang, 2), (jika, 2), (letak, ...\n",
      "13                                          [(baik, 1)]\n",
      "14                           [(kurang, 1), (update, 1)]\n",
      "15                                          [(baik, 1)]\n",
      "16                              [(cukup, 1), (baik, 1)]\n",
      "17    [(website, 1), (yang, 1), (tampilannya, 1), (k...\n",
      "18                                         [(bagus, 1)]\n",
      "19                       [(sangat, 1), (bermanfaat, 1)]\n",
      "20    [(bagus, 1), (sangat, 1), (bermanfaat, 1), (da...\n",
      "21                                    [(interaktif, 1)]\n",
      "22                                         [(bagus, 1)]\n",
      "23    [(yang, 2), (pendapat, 1), (saya, 1), (tentang...\n",
      "24    [(dan, 2), (website, 1), (institut, 1), (asia,...\n",
      "Name: Question1_tokens_fdist, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# NLTK calc frequency distribution\n",
    "def freqDist_wrapper(text):\n",
    "    return FreqDist(text)\n",
    "dataSB['Question1_tokens_fdist'] = dataSB['Question1_tokens'].apply(freqDist_wrapper)\n",
    "\n",
    "print('Frequency Tokens : \\n')\n",
    "print(dataSB['Question1_tokens_fdist'].head(25).apply(lambda x: x.most_common()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thaar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [website, menarik, mudah, dipahami, pengguna, ...\n",
      "1                                     [variatif, bagus]\n",
      "2                                       [keren, banget]\n",
      "3     [responsive, menu, sebelah, kanan, background,...\n",
      "4             [lumayan, responsif, adress, map, tampil]\n",
      "5                                                    []\n",
      "6                                     [lumayang, bagus]\n",
      "7                                         [kadang, bug]\n",
      "8                  [desainnya, menarik, user, friendly]\n",
      "9     [pendapat, website, informasi, kampus, asia, m...\n",
      "10                                              [bagus]\n",
      "11    [mengakses, website, kampus, menunya, lumayan,...\n",
      "12    [letak, navbar, efektif, disebelah, kanan, scr...\n",
      "13                                                   []\n",
      "14                                             [update]\n",
      "15                                                   []\n",
      "16                                                   []\n",
      "17             [website, tampilannya, kekinian, modern]\n",
      "18                                              [bagus]\n",
      "19                                         [bermanfaat]\n",
      "20                   [bagus, bermanfaat, menginspirasi]\n",
      "21                                         [interaktif]\n",
      "22                                              [bagus]\n",
      "23    [pendapat, kampus, institut, asia, malang, bag...\n",
      "24      [website, institut, asia, bagus, simple, ribet]\n",
      "Name: Question1_tokens_WSW, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "\n",
    "# ---------------------------- manualy add stopword  ------------------------------------\n",
    "# append additional stopword\n",
    "list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       '&amp', 'yah', 'bisnis', 'pandemi', 'indonesia'])\n",
    "\n",
    "# convert list to dictionary\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "#remove stopword pada list token\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "dataSB['Question1_tokens_WSW'] = dataSB['Question1_tokens'].apply(stopwords_removal) \n",
    "\n",
    "print(dataSB['Question1_tokens_WSW'].head(25))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [website, menarik, mudah, dipahami, pengguna, ...\n",
       "1                                     [variatif, bagus]\n",
       "2                                       [keren, banget]\n",
       "3     [responsive, menu, sebelah, kanan, background,...\n",
       "4             [lumayan, responsif, adress, map, tampil]\n",
       "5                                                    []\n",
       "6                                     [lumayang, bagus]\n",
       "7                                         [kadang, bug]\n",
       "8                  [desainnya, menarik, user, friendly]\n",
       "9     [pendapat, website, informasi, kampus, asia, m...\n",
       "10                                              [bagus]\n",
       "11    [mengakses, website, kampus, menunya, lumayan,...\n",
       "12    [letak, navbar, efektif, disebelah, kanan, scr...\n",
       "13                                                   []\n",
       "14                                             [update]\n",
       "15                                                   []\n",
       "16                                                   []\n",
       "17             [website, tampilannya, kekinian, modern]\n",
       "18                                              [bagus]\n",
       "19                                         [bermanfaat]\n",
       "20                   [bagus, bermanfaat, menginspirasi]\n",
       "21                                         [interaktif]\n",
       "22                                              [bagus]\n",
       "23    [pendapat, kampus, institut, asia, malang, bag...\n",
       "24      [website, institut, asia, bagus, simple, ribet]\n",
       "Name: Question1_normalized, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizad_word = pd.read_excel('/Users/thaar/Downloads/lda_topModeling_testing-main (2)/lda_topModeling_testing-main/testing 3/lda_test/content/dataKuisoner.xlsx', engine='openpyxl')  # lokasi file\n",
    "\n",
    "normalizad_word_dict = {}\n",
    "\n",
    "for index, row in normalizad_word.iterrows():\n",
    "    if row[0] not in normalizad_word_dict:\n",
    "        normalizad_word_dict[row[0]] = row[1]\n",
    "\n",
    "\n",
    "def normalized_term(document):\n",
    "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
    "\n",
    "\n",
    "dataSB['Question1_normalized'] = dataSB['Question1_tokens_WSW'].apply(\n",
    "    normalized_term)\n",
    "\n",
    "dataSB['Question1_normalized'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    }
   ],
   "source": [
    "# import Sastrawi package\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import swifter\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "    \n",
    "term_dict = {}\n",
    "\n",
    "for document in dataSB['Question1_normalized']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "\n",
    "print(len(term_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "\n",
    "    # untuk melihat hasilnya silahkan jalankan baris di bawah ini\n",
    "    # print(term,\":\" ,term_dict[term])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909d8a3e4c22480fadee1b1a54777577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [website, tarik, mudah, paham, guna, sera, use...\n",
      "1                                     [variatif, bagus]\n",
      "2                                       [keren, banget]\n",
      "3     [responsive, menu, belah, kanan, background, p...\n",
      "4             [lumayan, responsif, adress, map, tampil]\n",
      "5                                                    []\n",
      "6                                     [lumayang, bagus]\n",
      "7                                         [kadang, bug]\n",
      "8                       [desain, tarik, user, friendly]\n",
      "9     [dapat, website, informasi, kampus, asia, mili...\n",
      "10                                              [bagus]\n",
      "11    [akses, website, kampus, menu, lumayan, lengka...\n",
      "12    [letak, navbar, efektif, belah, kanan, scroll,...\n",
      "13                                                   []\n",
      "14                                             [update]\n",
      "15                                                   []\n",
      "16                                                   []\n",
      "17                      [website, tampil, kini, modern]\n",
      "18                                              [bagus]\n",
      "19                                            [manfaat]\n",
      "20                          [bagus, manfaat, inspirasi]\n",
      "21                                         [interaktif]\n",
      "22                                              [bagus]\n",
      "23    [dapat, kampus, institut, asia, malang, bagus,...\n",
      "24      [website, institut, asia, bagus, simple, ribet]\n",
      "25                              [bagus, tarik, kembang]\n",
      "26    [tampi, mudah, tarik, bantu, cari, informasi, ...\n",
      "27    [prestasi, mahasiswa, blum, tampil, link, sosm...\n",
      "28             [desain, bagus, padu, warna, pas, jenuh]\n",
      "29    [performa, cepat, komunikasi, akses, baik, opt...\n",
      "30                               [ui, ux, mudah, paham]\n",
      "31    [design, webnya, bagus, alangkah, baik, tambah...\n",
      "Name: Question1_tokens_stemmed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# apply stemmed term to dataframe\n",
    "\n",
    "\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "dataSB['Question1_tokens_stemmed'] = dataSB['Question1_normalized'].swifter.apply(get_stemmed_term)\n",
    "\n",
    "print(dataSB['Question1_tokens_stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [website, tarik, mudah, paham, sera, user, fri...\n",
      "1                                    [variatif, bagus]\n",
      "2                                      [keren, banget]\n",
      "3    [responsive, menu, belah, kanan, background, p...\n",
      "4            [lumayan, responsif, adress, map, tampil]\n",
      "Name: Question1_tokens_stemmed2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#stopwords #2\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "\n",
    "# ---------------------------- manualy add stopword  ------------------------------------\n",
    "# append additional stopword\n",
    "list_stopwords.extend([\"ada\", \"tan\", \"ton\", \"pt\", \"komentar\", \"juta\", \"unit\", \"menang\", \"artikel\",\n",
    "                       \"smartphone\", \"tagar\", \"sedia\", \"kaskus\", \"seksi\"])\n",
    "\n",
    "# convert list to dictionary\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "# remove stopword pada list token\n",
    "\n",
    "\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "\n",
    "dataSB['Question1_tokens_stemmed2'] = dataSB['Question1_tokens_stemmed'].apply(stopwords_removal)\n",
    "\n",
    "print(dataSB['Question1_tokens_stemmed2'].head())\n",
    "# print(dataSB['Question1_tokens_stemmed2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['design', 'webnya', 'bagus', 'alangkah', 'baiknya']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(dataSB)):\n",
    "    a = dataSB.iloc[i][0]\n",
    "    document.append(a)\n",
    "\n",
    "document[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [website, tarik, mudah, paham, guna, sera, use...\n",
       "1                                     [variatif, bagus]\n",
       "2                                       [keren, banget]\n",
       "3     [responsive, menu, belah, kanan, background, p...\n",
       "4             [lumayan, responsif, adress, map, tampil]\n",
       "5                                                    []\n",
       "6                                     [lumayang, bagus]\n",
       "7                                         [kadang, bug]\n",
       "8                       [desain, tarik, user, friendly]\n",
       "9     [dapat, website, informasi, kampus, asia, mili...\n",
       "10                                              [bagus]\n",
       "11    [akses, website, kampus, menu, lumayan, lengka...\n",
       "12    [letak, navbar, efektif, belah, kanan, scroll,...\n",
       "13                                                   []\n",
       "14                                             [update]\n",
       "15                                                   []\n",
       "16                                                   []\n",
       "17                      [website, tampil, kini, modern]\n",
       "18                                              [bagus]\n",
       "19                                            [manfaat]\n",
       "20                          [bagus, manfaat, inspirasi]\n",
       "21                                         [interaktif]\n",
       "22                                              [bagus]\n",
       "23    [dapat, kampus, institut, asia, malang, bagus,...\n",
       "24      [website, institut, asia, bagus, simple, ribet]\n",
       "25                              [bagus, tarik, kembang]\n",
       "26    [tampi, mudah, tarik, bantu, cari, informasi, ...\n",
       "27    [prestasi, mahasiswa, blum, tampil, link, sosm...\n",
       "28             [desain, bagus, padu, warna, pas, jenuh]\n",
       "29    [performa, cepat, komunikasi, akses, baik, opt...\n",
       "30                               [ui, ux, mudah, paham]\n",
       "31    [design, webnya, bagus, alangkah, baik, tambah...\n",
       "Name: Question1_tokens_stemmed, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_clean = dataSB['Question1_tokens_stemmed']\n",
    "doc_clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA model using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<115 unique tokens: ['friendly', 'guna', 'mudah', 'paham', 'sera']...>\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "print(dictionary)\n",
    "\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "total_topics = 3  # jumlah topik yang akan di extract\n",
    "number_words = 10  # jumlah kata per topik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.054*\"kampus\" + 0.039*\"bagus\" + 0.039*\"tampil\" + 0.032*\"informasi\" + 0.028*\"website\" + 0.024*\"asia\" + 0.017*\"tarik\" + 0.017*\"dapat\" + 0.017*\"mahasiswa\" + 0.017*\"prestasi\"'),\n",
       " (1,\n",
       "  '0.062*\"bagus\" + 0.032*\"kampus\" + 0.032*\"tarik\" + 0.023*\"asia\" + 0.023*\"mudah\" + 0.023*\"navbar\" + 0.023*\"paham\" + 0.023*\"desain\" + 0.023*\"user\" + 0.023*\"friendly\"'),\n",
       " (2,\n",
       "  '0.035*\"menu\" + 0.035*\"putih\" + 0.035*\"lengkap\" + 0.020*\"lumayan\" + 0.020*\"website\" + 0.020*\"tampil\" + 0.020*\"tarik\" + 0.020*\"friendly\" + 0.020*\"user\" + 0.020*\"akses\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running and Trainign LDA model on the document term matrix.\n",
    "lda_model = Lda(doc_term_matrix, num_topics=total_topics,\n",
    "                id2word=dictionary, passes=50)\n",
    "\n",
    "lda_model.show_topics(num_topics=total_topics, num_words=number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         word  topic_id  importance  word_count\n",
      "0      kampus         0    0.053707          11\n",
      "1       bagus         0    0.038764          11\n",
      "2      tampil         0    0.038637           6\n",
      "3   informasi         0    0.031695           5\n",
      "4     website         0    0.028231           6\n",
      "5        asia         0    0.024320           5\n",
      "6       tarik         0    0.017081           6\n",
      "7       dapat         0    0.017057           2\n",
      "8   mahasiswa         0    0.017056           2\n",
      "9    prestasi         0    0.017056           2\n",
      "10      bagus         1    0.061938          11\n",
      "11     kampus         1    0.032446          11\n",
      "12      tarik         1    0.032295           6\n",
      "13       asia         1    0.022781           5\n",
      "14      mudah         1    0.022741           3\n",
      "15     navbar         1    0.022713           2\n",
      "16      paham         1    0.022708           2\n",
      "17     desain         1    0.022707           2\n",
      "18       user         1    0.022602           3\n",
      "19   friendly         1    0.022602           3\n",
      "20       menu         2    0.034820           3\n",
      "21      putih         2    0.034806           3\n",
      "22    lengkap         2    0.034770           3\n",
      "23    lumayan         2    0.020106           2\n",
      "24    website         2    0.019995           6\n",
      "25     tampil         2    0.019948           6\n",
      "26      tarik         2    0.019947           6\n",
      "27   friendly         2    0.019917           3\n",
      "28       user         2    0.019917           3\n",
      "29      akses         2    0.019831           2\n"
     ]
    }
   ],
   "source": [
    "# Word Count of Topic Keywords\n",
    "from collections import Counter\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in doc_clean for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i, weight, counter[word]])\n",
    "\n",
    "df_imp_wcount = pd.DataFrame(\n",
    "    out, columns=['word', 'topic_id', 'importance', 'word_count'])\n",
    "print(df_imp_wcount)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simpan Ke Local Drive File df_imp_wcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedisimpan='/Users/thaar/Downloads/lda_topModeling_testing-main (2)/lda_topModeling_testing-main/testing 3/lda_test/content/df_imp_wcount.xlsx'\n",
    "df_imp_wcount.to_excel(filedisimpan, index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=doc_term_matrix, texts=document):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series(\n",
    "                    [int(topic_num), round(prop_topic, 4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic','Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return (sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
      "0             0             1.0              0.9213   \n",
      "1             1             0.0              0.7528   \n",
      "2             2             1.0              0.7759   \n",
      "3             3             2.0              0.9308   \n",
      "4             4             0.0              0.8826   \n",
      "5             5             0.0              0.3333   \n",
      "6             6             0.0              0.7527   \n",
      "7             7             0.0              0.7749   \n",
      "8             8             1.0              0.8593   \n",
      "9             9             0.0              0.9534   \n",
      "10           10             1.0              0.6427   \n",
      "11           11             2.0              0.9607   \n",
      "12           12             1.0              0.9497   \n",
      "13           13             0.0              0.3333   \n",
      "14           14             2.0              0.6472   \n",
      "15           15             0.0              0.3333   \n",
      "16           16             0.0              0.3333   \n",
      "17           17             1.0              0.5481   \n",
      "18           18             1.0              0.6427   \n",
      "19           19             0.0              0.6646   \n",
      "20           20             0.0              0.8235   \n",
      "21           21             2.0              0.6652   \n",
      "22           22             1.0              0.6426   \n",
      "23           23             0.0              0.9641   \n",
      "24           24             0.0              0.8994   \n",
      "\n",
      "                                             Keywords  \\\n",
      "0   bagus, kampus, tarik, asia, mudah, navbar, pah...   \n",
      "1   kampus, bagus, tampil, informasi, website, asi...   \n",
      "2   bagus, kampus, tarik, asia, mudah, navbar, pah...   \n",
      "3   menu, putih, lengkap, lumayan, website, tampil...   \n",
      "4   kampus, bagus, tampil, informasi, website, asi...   \n",
      "5   kampus, bagus, tampil, informasi, website, asi...   \n",
      "6   kampus, bagus, tampil, informasi, website, asi...   \n",
      "7   kampus, bagus, tampil, informasi, website, asi...   \n",
      "8   bagus, kampus, tarik, asia, mudah, navbar, pah...   \n",
      "9   kampus, bagus, tampil, informasi, website, asi...   \n",
      "10  bagus, kampus, tarik, asia, mudah, navbar, pah...   \n",
      "11  menu, putih, lengkap, lumayan, website, tampil...   \n",
      "12  bagus, kampus, tarik, asia, mudah, navbar, pah...   \n",
      "13  kampus, bagus, tampil, informasi, website, asi...   \n",
      "14  menu, putih, lengkap, lumayan, website, tampil...   \n",
      "15  kampus, bagus, tampil, informasi, website, asi...   \n",
      "16  kampus, bagus, tampil, informasi, website, asi...   \n",
      "17  bagus, kampus, tarik, asia, mudah, navbar, pah...   \n",
      "18  bagus, kampus, tarik, asia, mudah, navbar, pah...   \n",
      "19  kampus, bagus, tampil, informasi, website, asi...   \n",
      "20  kampus, bagus, tampil, informasi, website, asi...   \n",
      "21  menu, putih, lengkap, lumayan, website, tampil...   \n",
      "22  bagus, kampus, tarik, asia, mudah, navbar, pah...   \n",
      "23  kampus, bagus, tampil, informasi, website, asi...   \n",
      "24  kampus, bagus, tampil, informasi, website, asi...   \n",
      "\n",
      "                                                 Text  \n",
      "0   [website, tarik, mudah, paham, guna, sera, use...  \n",
      "1                                   [variatif, bagus]  \n",
      "2                                     [keren, banget]  \n",
      "3   [responsive, menu, belah, kanan, background, p...  \n",
      "4           [lumayan, responsif, adress, map, tampil]  \n",
      "5                                                  []  \n",
      "6                                   [lumayang, bagus]  \n",
      "7                                       [kadang, bug]  \n",
      "8                     [desain, tarik, user, friendly]  \n",
      "9   [dapat, website, informasi, kampus, asia, mili...  \n",
      "10                                            [bagus]  \n",
      "11  [akses, website, kampus, menu, lumayan, lengka...  \n",
      "12  [letak, navbar, efektif, belah, kanan, scroll,...  \n",
      "13                                                 []  \n",
      "14                                           [update]  \n",
      "15                                                 []  \n",
      "16                                                 []  \n",
      "17                    [website, tampil, kini, modern]  \n",
      "18                                            [bagus]  \n",
      "19                                          [manfaat]  \n",
      "20                        [bagus, manfaat, inspirasi]  \n",
      "21                                       [interaktif]  \n",
      "22                                            [bagus]  \n",
      "23  [dapat, kampus, institut, asia, malang, bagus,...  \n",
      "24    [website, institut, asia, bagus, simple, ribet]  \n"
     ]
    }
   ],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(\n",
    "    ldamodel=lda_model, corpus=doc_term_matrix, texts=doc_clean)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = [\n",
    "    'Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "print(df_dominant_topic.head(25))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simpan Ke Local Drive File df_dominant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedisimpan='/Users/thaar/Downloads/lda_topModeling_testing-main (2)/lda_topModeling_testing-main/testing 3/lda_test/content/df_dominant_topic.xlsx'\n",
    "df_dominant_topic.to_excel(filedisimpan, index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "LDAvis_data_filepath = os.path.join('ldavis_prepared_'+str(total_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Could not pickle object as excessively deep recursion required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pickle5\\pickle.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    484\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_framing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pickle5\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[1;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pickle5\\pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m             \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m             \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pickle5\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    569\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_global\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m                     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pickle5\\pickle.py\u001b[0m in \u001b[0;36msave_global\u001b[1;34m(self, obj, name)\u001b[0m\n\u001b[0;32m   1092\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1093\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1094\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "... last 4 frames repeated, from the frame below ...\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pickle5\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[1;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18212\\2587288048.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# proses ini mungkin agak lama\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mLDAvis_prepared\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLDAvis_data_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLDAvis_prepared\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pyLDAvis\\gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \"\"\"\n\u001b[0;32m    123\u001b[0m     \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[0;32m    440\u001b[0m     topic_info = _topic_info(topic_term_dists, topic_proportion,\n\u001b[0;32m    441\u001b[0m                              \u001b[0mterm_frequency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m                              n_jobs, start_index)\n\u001b[0m\u001b[0;32m    443\u001b[0m     \u001b[0mtoken_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_token_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[0mtopic_coordinates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_topic_coordinates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_term_dists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36m_topic_info\u001b[1;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs, start_index)\u001b[0m\n\u001b[0;32m    277\u001b[0m     top_terms = pd.concat(Parallel(n_jobs=n_jobs)\n\u001b[0;32m    278\u001b[0m                           (delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls)\n\u001b[1;32m--> 279\u001b[1;33m                           for ls in _job_chunks(lambda_seq, n_jobs)))\n\u001b[0m\u001b[0;32m    280\u001b[0m     \u001b[0mtopic_dfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_top_term_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_terms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdefault_term_info\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_dfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1083\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1085\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1087\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 901\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    902\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    554\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m         \u001b[0mfuture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSafeFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m         \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_future_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\externals\\loky\\reusable_executor.py\u001b[0m in \u001b[0;36msubmit\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_submit_resize_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_resize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_workers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\u001b[0m in \u001b[0;36msubmit\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1147\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_executor_manager_thread_wakeup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwakeup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_executor_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m     \u001b[0msubmit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExecutor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\u001b[0m in \u001b[0;36m_ensure_executor_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_processes_management_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_processes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_workers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1123\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adjust_process_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1124\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_executor_manager_thread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\u001b[0m in \u001b[0;36m_adjust_process_count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1109\u001b[0m                 \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_process_worker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_worker_exit_lock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mworker_exit_lock\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1111\u001b[1;33m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1112\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_processes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         mp.util.debug(\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\externals\\loky\\backend\\process.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_loky_posix\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\externals\\loky\\backend\\popen_loky_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m                     \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m                     \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, reducers, protocol)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using _LokyPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0m_LokyPickler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m     \u001b[0m_LokyPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreducers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreducers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    637\u001b[0m                     \u001b[1;34m\"required.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m                 )\n\u001b[1;32m--> 639\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPicklingError\u001b[0m: Could not pickle object as excessively deep recursion required."
     ]
    }
   ],
   "source": [
    "# proses ini mungkin agak lama\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "    with open(LDAvis_data_filepath, 'wb') as e:\n",
    "        pickle.dump(LDAvis_prepared, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(LDAvis_prepared, '/Users/thaar/Downloads/lda_topModeling_testing-main (2)/lda_topModeling_testing-main/testing 3/lda_test/content/ldavis_prepared_'+ str(total_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proses ini mungkin agak lama\n",
    "LDAvis_prepared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e365d12e1b12276c01e710601ff33b16dd04a8fffe9e0504073f0b1e615b6389"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
