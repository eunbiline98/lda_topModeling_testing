{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (6.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyldavis==3.2.1 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.2.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.9.2 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyldavis==3.2.1) (1.21.5)\n",
      "Requirement already satisfied: numexpr in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyldavis==3.2.1) (2.8.4)\n",
      "Requirement already satisfied: pandas>=0.17.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyldavis==3.2.1) (1.1.5)\n",
      "Requirement already satisfied: joblib>=0.8.4 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyldavis==3.2.1) (1.2.0)\n",
      "Requirement already satisfied: wheel>=0.23.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyldavis==3.2.1) (0.37.1)\n",
      "Requirement already satisfied: scipy>=0.18.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyldavis==3.2.1) (1.7.3)\n",
      "Requirement already satisfied: future in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyldavis==3.2.1) (0.18.2)\n",
      "Requirement already satisfied: funcy in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyldavis==3.2.1) (1.17)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyldavis==3.2.1) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jinja2>=2.7.2->pyldavis==3.2.1) (2.1.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas>=0.17.0->pyldavis==3.2.1) (2022.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas>=0.17.0->pyldavis==3.2.1) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=0.17.0->pyldavis==3.2.1) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install pyldavis==3.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Sastrawi in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swifter in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from swifter) (2.2.0)\n",
      "Requirement already satisfied: bleach>=3.1.1 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from swifter) (5.0.1)\n",
      "Requirement already satisfied: psutil>=5.6.6 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from swifter) (5.9.0)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from swifter) (1.1.5)\n",
      "Requirement already satisfied: parso>0.4.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from swifter) (0.8.3)\n",
      "Requirement already satisfied: dask[dataframe]>=2.10.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from swifter) (2022.2.0)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from swifter) (4.64.1)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from swifter) (8.0.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from bleach>=3.1.1->swifter) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from bleach>=3.1.1->swifter) (0.5.1)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (2022.11.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (6.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (1.3.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (0.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (21.3)\n",
      "Requirement already satisfied: numpy>=1.18 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (1.21.5)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (4.0.3)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (3.0.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (5.6.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (6.16.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (7.34.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas>=1.0.0->swifter) (2022.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tqdm>=4.33.0->swifter) (0.4.4)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (24.0.1)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (1.6.4)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (6.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (0.1.6)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (7.4.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (1.5.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.7.5)\n",
      "Requirement already satisfied: pygments in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (2.13.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.18.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (63.4.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (3.0.33)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from packaging>=20.0->dask[dataframe]>=2.10.0->swifter) (3.0.9)\n",
      "Requirement already satisfied: locket in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from partd>=0.3.10->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (4.12.0)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (0.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.2.5)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (304)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\thaar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install swifter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Library Read File Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Folding Result : \n",
      "\n",
      "0     sts\n",
      "1      ts\n",
      "2     sts\n",
      "3      ss\n",
      "4      cs\n",
      "5     sts\n",
      "6     sts\n",
      "7      ss\n",
      "8      ts\n",
      "9      ss\n",
      "10    sts\n",
      "11     ts\n",
      "12     ts\n",
      "13     cs\n",
      "14     cs\n",
      "15     ss\n",
      "16    sts\n",
      "17     cs\n",
      "18    sts\n",
      "19     cs\n",
      "20     cs\n",
      "21     ss\n",
      "22     cs\n",
      "23     ts\n",
      "24     cs\n",
      "Name: Question1, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\compat\\_optional.py:117: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if distutils.version.LooseVersion(version) < minimum_version:\n",
      "c:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "dataSB = pd.read_excel(\n",
    "    '/Users/thaar/Documents/work/Project Ariq/LDA/testing 3/lda_test/content/dataDummy.xlsx', engine='openpyxl')  # lokasi file\n",
    "dataSB.head()\n",
    "dataSB['Question1'] = dataSB['Question1'].str.lower()\n",
    "\n",
    "print('Case Folding Result : \\n')\n",
    "print(dataSB['Question1'].head(25))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import re #regex library\n",
    "\n",
    "# import word_tokenize & FreqDist from NLTK\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:52: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:14: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:52: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:14: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:52: DeprecationWarning: invalid escape sequence \\s\n",
      "C:\\Users\\thaar\\AppData\\Local\\Temp\\ipykernel_23232\\1723946190.py:14: DeprecationWarning: invalid escape sequence \\w\n",
      "  re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
      "C:\\Users\\thaar\\AppData\\Local\\Temp\\ipykernel_23232\\1723946190.py:52: DeprecationWarning: invalid escape sequence \\s\n",
      "  return re.sub('\\s+', ' ', text)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\thaar\\AppData\\Roaming\\nltk_data..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Result : \n",
      "\n",
      "0    [sts]\n",
      "1     [ts]\n",
      "2    [sts]\n",
      "3     [ss]\n",
      "4     [cs]\n",
      "Name: Question1_tokens, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ------ Tokenizing ---------\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def remove_tweet_special(text):\n",
    "    # remove tab, new line, ans back slice\n",
    "    text = str(text).replace('\\\\t', \" \").replace(\n",
    "        '\\\\n', \" \").replace('\\\\u', \" \").replace('\\\\', \"\")\n",
    "    # remove non ASCII (emoticon, chinese word, .etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # remove mention, link, hashtag\n",
    "    text = ' '.join(\n",
    "        re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
    "    # remove incomplete URL\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "\n",
    "\n",
    "dataSB['Question1'] = dataSB['Question1'].apply(remove_tweet_special)\n",
    "\n",
    "# remove number\n",
    "\n",
    "\n",
    "def remove_number(text):\n",
    "    return re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "\n",
    "dataSB['Question1'] = dataSB['Question1'].apply(remove_number)\n",
    "\n",
    "# remove punctuation\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "\n",
    "dataSB['Question1'] = dataSB['Question1'].apply(remove_punctuation)\n",
    "\n",
    "# remove whitespace leading & trailing\n",
    "\n",
    "\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "dataSB['Question1'] = dataSB['Question1'].apply(remove_whitespace_LT)\n",
    "\n",
    "# remove multiple whitespace into single whitespace\n",
    "\n",
    "\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+', ' ', text)\n",
    "\n",
    "\n",
    "dataSB['Question1'] = dataSB['Question1'].apply(remove_whitespace_multiple)\n",
    "\n",
    "# remove single char\n",
    "\n",
    "\n",
    "def remove_singl_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "\n",
    "dataSB['Question1'] = dataSB['Question1'].apply(remove_singl_char)\n",
    "\n",
    "# NLTK word tokenize\n",
    "\n",
    "\n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "dataSB['Question1_tokens'] = dataSB['Question1'].apply(word_tokenize_wrapper)\n",
    "\n",
    "print('Tokenizing Result : \\n')\n",
    "print(dataSB['Question1_tokens'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Tokens : \n",
      "\n",
      "0    [(sts, 1)]\n",
      "1     [(ts, 1)]\n",
      "2    [(sts, 1)]\n",
      "3     [(ss, 1)]\n",
      "4     [(cs, 1)]\n",
      "Name: Question1_tokens_fdist, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# NLTK calc frequency distribution\n",
    "def freqDist_wrapper(text):\n",
    "    return FreqDist(text)\n",
    "dataSB['Question1_tokens_fdist'] = dataSB['Question1_tokens'].apply(freqDist_wrapper)\n",
    "\n",
    "print('Frequency Tokens : \\n')\n",
    "print(dataSB['Question1_tokens_fdist'].head().apply(lambda x: x.most_common()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thaar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [sts]\n",
      "1     [ts]\n",
      "2    [sts]\n",
      "3     [ss]\n",
      "4     [cs]\n",
      "Name: Question1_tokens_WSW, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "\n",
    "# ---------------------------- manualy add stopword  ------------------------------------\n",
    "# append additional stopword\n",
    "list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       '&amp', 'yah', 'bisnis', 'pandemi', 'indonesia'])\n",
    "\n",
    "# convert list to dictionary\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "#remove stopword pada list token\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "dataSB['Question1_tokens_WSW'] = dataSB['Question1_tokens'].apply(stopwords_removal) \n",
    "\n",
    "print(dataSB['Question1_tokens_WSW'].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\compat\\_optional.py:117: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if distutils.version.LooseVersion(version) < minimum_version:\n",
      "c:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     [sts]\n",
       "1      [ts]\n",
       "2     [sts]\n",
       "3      [ss]\n",
       "4      [cs]\n",
       "5     [sts]\n",
       "6     [sts]\n",
       "7      [ss]\n",
       "8      [ts]\n",
       "9      [ss]\n",
       "10    [sts]\n",
       "11     [ts]\n",
       "12     [ts]\n",
       "13     [cs]\n",
       "14     [cs]\n",
       "15     [ss]\n",
       "16    [sts]\n",
       "17     [cs]\n",
       "18    [sts]\n",
       "19     [cs]\n",
       "20     [cs]\n",
       "21     [ss]\n",
       "22     [cs]\n",
       "23     [ts]\n",
       "24     [cs]\n",
       "Name: Question1_normalized, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizad_word = pd.read_excel(\n",
    "    '/Users/thaar/Documents/work/Project Ariq/LDA/testing 3/lda_test/content/dataBerita.xlsx', engine='openpyxl')  # lokasi file\n",
    "\n",
    "normalizad_word_dict = {}\n",
    "\n",
    "for index, row in normalizad_word.iterrows():\n",
    "    if row[0] not in normalizad_word_dict:\n",
    "        normalizad_word_dict[row[0]] = row[1]\n",
    "\n",
    "\n",
    "def normalized_term(document):\n",
    "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
    "\n",
    "\n",
    "dataSB['Question1_normalized'] = dataSB['Question1_tokens_WSW'].apply(\n",
    "    normalized_term)\n",
    "\n",
    "dataSB['Question1_normalized'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\compat\\_optional.py:117: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if distutils.version.LooseVersion(version) < minimum_version:\n",
      "c:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "# import Sastrawi package\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import swifter\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "    \n",
    "term_dict = {}\n",
    "\n",
    "for document in dataSB['Question1_normalized']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "\n",
    "print(len(term_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "\n",
    "    # untuk melihat hasilnya silahkan jalankan baris di bawah ini\n",
    "    # print(term,\":\" ,term_dict[term])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ad09316bea463eafd0f738661c4d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [sts]\n",
      "1       [ts]\n",
      "2      [sts]\n",
      "3       [ss]\n",
      "4       [cs]\n",
      "       ...  \n",
      "994       []\n",
      "995       []\n",
      "996       []\n",
      "997       []\n",
      "998       []\n",
      "Name: Question1_tokens_stemmed, Length: 999, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# apply stemmed term to dataframe\n",
    "\n",
    "\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "dataSB['Question1_tokens_stemmed'] = dataSB['Question1_normalized'].swifter.apply(get_stemmed_term)\n",
    "\n",
    "print(dataSB['Question1_tokens_stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [sts]\n",
      "1     [ts]\n",
      "2    [sts]\n",
      "3     [ss]\n",
      "4     [cs]\n",
      "Name: Question1_tokens_stemmed2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#stopwords #2\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "\n",
    "# ---------------------------- manualy add stopword  ------------------------------------\n",
    "# append additional stopword\n",
    "list_stopwords.extend([\"ada\", \"tan\", \"ton\", \"pt\", \"komentar\", \"juta\", \"unit\", \"menang\", \"artikel\",\n",
    "                       \"smartphone\", \"tagar\", \"sedia\", \"kaskus\", \"seksi\"])\n",
    "\n",
    "# convert list to dictionary\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "# remove stopword pada list token\n",
    "\n",
    "\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "\n",
    "dataSB['Question1_tokens_stemmed2'] = dataSB['Question1_tokens_stemmed'].apply(stopwords_removal)\n",
    "\n",
    "print(dataSB['Question1_tokens_stemmed2'].head())\n",
    "# print(dataSB['Question1_tokens_stemmed2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sinb', 'Ari', 'Jessica', 'Jason', 'Naviza']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(dataSB)):\n",
    "    a = dataSB.iloc[i][0]\n",
    "    document.append(a)\n",
    "\n",
    "document[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [sts]\n",
       "1       [ts]\n",
       "2      [sts]\n",
       "3       [ss]\n",
       "4       [cs]\n",
       "       ...  \n",
       "994       []\n",
       "995       []\n",
       "996       []\n",
       "997       []\n",
       "998       []\n",
       "Name: Question1_tokens_stemmed, Length: 999, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_clean = dataSB['Question1_tokens_stemmed']\n",
    "doc_clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA model using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<4 unique tokens: ['sts', 'ts', 'ss', 'cs']>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "print(dictionary)\n",
    "\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "total_topics = 3  # jumlah topik yang akan di extract\n",
    "number_words = 10  # jumlah kata per topik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.840*\"ts\" + 0.054*\"sts\" + 0.054*\"cs\" + 0.053*\"ss\"'),\n",
       " (1, '0.840*\"ss\" + 0.054*\"sts\" + 0.054*\"cs\" + 0.053*\"ts\"'),\n",
       " (2, '0.510*\"cs\" + 0.449*\"sts\" + 0.020*\"ss\" + 0.020*\"ts\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running and Trainign LDA model on the document term matrix.\n",
    "lda_model = Lda(doc_term_matrix, num_topics=total_topics,\n",
    "                id2word=dictionary, passes=50)\n",
    "\n",
    "lda_model.show_topics(num_topics=total_topics, num_words=number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word  topic_id  importance  word_count\n",
      "0    ts         0    0.840004           5\n",
      "1   sts         0    0.053553           7\n",
      "2    cs         0    0.053548           8\n",
      "3    ss         0    0.052895           5\n",
      "4    ss         1    0.840004           5\n",
      "5   sts         1    0.053553           7\n",
      "6    cs         1    0.053548           8\n",
      "7    ts         1    0.052895           5\n",
      "8    cs         2    0.510170           8\n",
      "9   sts         2    0.448851           7\n",
      "10   ss         2    0.020490           5\n",
      "11   ts         2    0.020490           5\n"
     ]
    }
   ],
   "source": [
    "# Word Count of Topic Keywords\n",
    "from collections import Counter\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in doc_clean for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i, weight, counter[word]])\n",
    "\n",
    "df_imp_wcount = pd.DataFrame(\n",
    "    out, columns=['word', 'topic_id', 'importance', 'word_count'])\n",
    "print(df_imp_wcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=doc_term_matrix, texts=document):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series(\n",
    "                    [int(topic_num), round(prop_topic, 4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic','Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return (sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Document_No  Dominant_Topic  Topic_Perc_Contrib         Keywords   Text\n",
      "0             0             2.0              0.6657  cs, sts, ss, ts  [sts]\n",
      "1             1             0.0              0.6664  ts, sts, cs, ss   [ts]\n",
      "2             2             2.0              0.6657  cs, sts, ss, ts  [sts]\n",
      "3             3             1.0              0.6664  ss, sts, cs, ts   [ss]\n",
      "4             4             2.0              0.6659  cs, sts, ss, ts   [cs]\n",
      "5             5             2.0              0.6657  cs, sts, ss, ts  [sts]\n",
      "6             6             2.0              0.6657  cs, sts, ss, ts  [sts]\n",
      "7             7             1.0              0.6664  ss, sts, cs, ts   [ss]\n",
      "8             8             0.0              0.6664  ts, sts, cs, ss   [ts]\n",
      "9             9             1.0              0.6664  ss, sts, cs, ts   [ss]\n",
      "10           10             2.0              0.6657  cs, sts, ss, ts  [sts]\n",
      "11           11             0.0              0.6664  ts, sts, cs, ss   [ts]\n",
      "12           12             0.0              0.6664  ts, sts, cs, ss   [ts]\n",
      "13           13             2.0              0.6659  cs, sts, ss, ts   [cs]\n",
      "14           14             2.0              0.6659  cs, sts, ss, ts   [cs]\n",
      "15           15             1.0              0.6664  ss, sts, cs, ts   [ss]\n",
      "16           16             2.0              0.6657  cs, sts, ss, ts  [sts]\n",
      "17           17             2.0              0.6659  cs, sts, ss, ts   [cs]\n",
      "18           18             2.0              0.6657  cs, sts, ss, ts  [sts]\n",
      "19           19             2.0              0.6659  cs, sts, ss, ts   [cs]\n",
      "20           20             2.0              0.6659  cs, sts, ss, ts   [cs]\n",
      "21           21             1.0              0.6664  ss, sts, cs, ts   [ss]\n",
      "22           22             2.0              0.6659  cs, sts, ss, ts   [cs]\n",
      "23           23             0.0              0.6664  ts, sts, cs, ss   [ts]\n",
      "24           24             2.0              0.6659  cs, sts, ss, ts   [cs]\n"
     ]
    }
   ],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(\n",
    "    ldamodel=lda_model, corpus=doc_term_matrix, texts=doc_clean)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = [\n",
    "    'Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "print(df_dominant_topic.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "LDAvis_data_filepath = os.path.join('/Users/thaar/Documents/work/Project Ariq/LDA/testing 3/lda_test'+str(total_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Could not pickle object as excessively deep recursion required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pickle5\\pickle.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    484\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_framing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pickle5\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[1;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pickle5\\pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m             \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m             \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pickle5\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    569\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_global\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m                     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pickle5\\pickle.py\u001b[0m in \u001b[0;36msave_global\u001b[1;34m(self, obj, name)\u001b[0m\n\u001b[0;32m   1092\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1093\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1094\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "... last 4 frames repeated, from the frame below ...\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pickle5\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[1;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23232\\2587288048.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# proses ini mungkin agak lama\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mLDAvis_prepared\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLDAvis_data_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLDAvis_prepared\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pyLDAvis\\gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \"\"\"\n\u001b[0;32m    123\u001b[0m     \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[0;32m    440\u001b[0m     topic_info = _topic_info(topic_term_dists, topic_proportion,\n\u001b[0;32m    441\u001b[0m                              \u001b[0mterm_frequency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m                              n_jobs, start_index)\n\u001b[0m\u001b[0;32m    443\u001b[0m     \u001b[0mtoken_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_token_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[0mtopic_coordinates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_topic_coordinates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_term_dists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36m_topic_info\u001b[1;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs, start_index)\u001b[0m\n\u001b[0;32m    277\u001b[0m     top_terms = pd.concat(Parallel(n_jobs=n_jobs)\n\u001b[0;32m    278\u001b[0m                           (delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls)\n\u001b[1;32m--> 279\u001b[1;33m                           for ls in _job_chunks(lambda_seq, n_jobs)))\n\u001b[0m\u001b[0;32m    280\u001b[0m     \u001b[0mtopic_dfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_top_term_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_terms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdefault_term_info\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_dfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1083\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1085\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1087\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 901\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    902\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    554\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m         \u001b[0mfuture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSafeFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m         \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_future_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\externals\\loky\\reusable_executor.py\u001b[0m in \u001b[0;36msubmit\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_submit_resize_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_resize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_workers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\u001b[0m in \u001b[0;36msubmit\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1147\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_executor_manager_thread_wakeup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwakeup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_executor_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m     \u001b[0msubmit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExecutor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\u001b[0m in \u001b[0;36m_ensure_executor_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_processes_management_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_processes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_workers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1123\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adjust_process_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1124\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_executor_manager_thread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\u001b[0m in \u001b[0;36m_adjust_process_count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1109\u001b[0m                 \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_process_worker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_worker_exit_lock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mworker_exit_lock\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1111\u001b[1;33m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1112\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_processes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         mp.util.debug(\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\externals\\loky\\backend\\process.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_loky_posix\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\externals\\loky\\backend\\popen_loky_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m                     \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m                     \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, reducers, protocol)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using _LokyPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0m_LokyPickler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m     \u001b[0m_LokyPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreducers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreducers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thaar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    637\u001b[0m                     \u001b[1;34m\"required.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m                 )\n\u001b[1;32m--> 639\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPicklingError\u001b[0m: Could not pickle object as excessively deep recursion required."
     ]
    }
   ],
   "source": [
    "# proses ini mungkin agak lama\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Document_No  Dominant_Topic  Topic_Perc_Contrib         Keywords   Text\n",
      "0             0             2.0              0.6664  sts, ss, cs, ts  [sts]\n",
      "1             1             1.0              0.6664  ts, ss, cs, sts   [ts]\n",
      "2             2             2.0              0.6664  sts, ss, cs, ts  [sts]\n",
      "3             3             0.0              0.6657  cs, ss, sts, ts   [ss]\n",
      "4             4             0.0              0.6661  cs, ss, sts, ts   [cs]\n",
      "5             5             2.0              0.6664  sts, ss, cs, ts  [sts]\n",
      "6             6             2.0              0.6664  sts, ss, cs, ts  [sts]\n",
      "7             7             0.0              0.6657  cs, ss, sts, ts   [ss]\n",
      "8             8             1.0              0.6664  ts, ss, cs, sts   [ts]\n",
      "9             9             0.0              0.6657  cs, ss, sts, ts   [ss]\n",
      "10           10             2.0              0.6664  sts, ss, cs, ts  [sts]\n",
      "11           11             1.0              0.6664  ts, ss, cs, sts   [ts]\n",
      "12           12             1.0              0.6664  ts, ss, cs, sts   [ts]\n",
      "13           13             0.0              0.6661  cs, ss, sts, ts   [cs]\n",
      "14           14             0.0              0.6661  cs, ss, sts, ts   [cs]\n",
      "15           15             0.0              0.6657  cs, ss, sts, ts   [ss]\n",
      "16           16             2.0              0.6664  sts, ss, cs, ts  [sts]\n",
      "17           17             0.0              0.6661  cs, ss, sts, ts   [cs]\n",
      "18           18             2.0              0.6664  sts, ss, cs, ts  [sts]\n",
      "19           19             0.0              0.6661  cs, ss, sts, ts   [cs]\n",
      "20           20             0.0              0.6661  cs, ss, sts, ts   [cs]\n",
      "21           21             0.0              0.6657  cs, ss, sts, ts   [ss]\n",
      "22           22             0.0              0.6661  cs, ss, sts, ts   [cs]\n",
      "23           23             1.0              0.6664  ts, ss, cs, sts   [ts]\n",
      "24           24             0.0              0.6661  cs, ss, sts, ts   [cs]\n"
     ]
    }
   ],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(\n",
    "    ldamodel=lda_model, corpus=doc_term_matrix, texts=doc_clean)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = [\n",
    "    'Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "print(df_dominant_topic.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LDAvis_prepared' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26864\\3474969589.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLDAvis_prepared\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'drive/My Drive/Colab Notebooks/LDA Indonesia/ldavis_prepared_'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_topics\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'.html'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'LDAvis_prepared' is not defined"
     ]
    }
   ],
   "source": [
    "pyLDAvis.save_html(LDAvis_prepared, 'drive/My Drive/Colab Notebooks/LDA Indonesia/ldavis_prepared_'+ str(total_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LDAvis_prepared' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26864\\931982742.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# proses ini mungkin agak lama\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mLDAvis_prepared\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'LDAvis_prepared' is not defined"
     ]
    }
   ],
   "source": [
    "# proses ini mungkin agak lama\n",
    "LDAvis_prepared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e365d12e1b12276c01e710601ff33b16dd04a8fffe9e0504073f0b1e615b6389"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
